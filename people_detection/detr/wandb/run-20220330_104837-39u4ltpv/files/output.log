Not using distributed mode
git:
  sha: 8a144f83a287f4d3fece4acdf073f387c5af387d, status: has uncommited changes, branch: main
Namespace(lr=0.0001, lr_backbone=1e-05, batch_size=2, weight_decay=0.0001, epochs=300, lr_drop=200, clip_max_norm=0.1, frozen_weights=None, backbone='resnet50', dilation=False, position_embedding='sine', enc_layers=6, dec_layers=6, dim_feedforward=2048, hidden_dim=256, dropout=0.1, nheads=8, num_queries=100, pre_norm=False, masks=False, aux_loss=True, set_cost_class=1, set_cost_bbox=5, set_cost_giou=2, mask_loss_coef=1, dice_loss_coef=1, bbox_loss_coef=5, giou_loss_coef=2, eos_coef=0.1, dataset_file='historical', data_path='../dataset/', coco_panoptic_path=None, remove_difficult=False, output_dir='', device='cuda', seed=42, resume='', start_epoch=0, eval=False, num_workers=2, world_size=1, dist_url='env://', distributed=False)
number of params: 41280266
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
Start training
C:\Users\chris\Desktop\SUS\detr\models\position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)
Epoch: [0]  [  0/134]  eta: 0:12:31  lr: 0.000100  class_error: 100.00  loss: 35.1240 (35.1240)  loss_ce: 2.0435 (2.0435)  loss_bbox: 2.6445 (2.6445)  loss_giou: 1.4059 (1.4059)  loss_ce_0: 2.0437 (2.0437)  loss_bbox_0: 2.5956 (2.5956)  loss_giou_0: 1.3166 (1.3166)  loss_ce_1: 1.8657 (1.8657)  loss_bbox_1: 2.4283 (2.4283)  loss_giou_1: 1.2442 (1.2442)  loss_ce_2: 2.4041 (2.4041)  loss_bbox_2: 2.5340 (2.5340)  loss_giou_2: 1.2544 (1.2544)  loss_ce_3: 1.7537 (1.7537)  loss_bbox_3: 2.5213 (2.5213)  loss_giou_3: 1.3055 (1.3055)  loss_ce_4: 1.8473 (1.8473)  loss_bbox_4: 2.5813 (2.5813)  loss_giou_4: 1.3344 (1.3344)  loss_ce_unscaled: 2.0435 (2.0435)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.5289 (0.5289)  loss_giou_unscaled: 0.7030 (0.7030)  cardinality_error_unscaled: 97.0000 (97.0000)  loss_ce_0_unscaled: 2.0437 (2.0437)  loss_bbox_0_unscaled: 0.5191 (0.5191)  loss_giou_0_unscaled: 0.6583 (0.6583)  cardinality_error_0_unscaled: 99.0000 (99.0000)  loss_ce_1_unscaled: 1.8657 (1.8657)  loss_bbox_1_unscaled: 0.4857 (0.4857)  loss_giou_1_unscaled: 0.6221 (0.6221)  cardinality_error_1_unscaled: 93.0000 (93.0000)  loss_ce_2_unscaled: 2.4041 (2.4041)  loss_bbox_2_unscaled: 0.5068 (0.5068)  loss_giou_2_unscaled: 0.6272 (0.6272)  cardinality_error_2_unscaled: 99.0000 (99.0000)  loss_ce_3_unscaled: 1.7537 (1.7537)  loss_bbox_3_unscaled: 0.5043 (0.5043)  loss_giou_3_unscaled: 0.6527 (0.6527)  cardinality_error_3_unscaled: 90.0000 (90.0000)  loss_ce_4_unscaled: 1.8473 (1.8473)  loss_bbox_4_unscaled: 0.5163 (0.5163)  loss_giou_4_unscaled: 0.6672 (0.6672)  cardinality_error_4_unscaled: 92.0000 (92.0000)  time: 5.6108  data: 2.7248  max mem: 1740
Epoch: [0]  [ 10/134]  eta: 0:01:45  lr: 0.000100  class_error: 100.00  loss: 35.1240 (35.9943)  loss_ce: 0.8608 (0.9116)  loss_bbox: 3.0481 (3.3408)  loss_giou: 1.7003 (1.7862)  loss_ce_0: 0.8398 (0.8757)  loss_bbox_0: 3.0245 (3.3536)  loss_giou_0: 1.7210 (1.7586)  loss_ce_1: 0.7657 (0.8497)  loss_bbox_1: 3.1130 (3.3266)  loss_giou_1: 1.7534 (1.7688)  loss_ce_2: 0.7772 (0.8918)  loss_bbox_2: 3.1273 (3.3558)  loss_giou_2: 1.7513 (1.7795)  loss_ce_3: 0.8100 (0.8511)  loss_bbox_3: 3.1113 (3.3431)  loss_giou_3: 1.7484 (1.7870)  loss_ce_4: 0.8502 (0.8933)  loss_bbox_4: 3.0406 (3.3379)  loss_giou_4: 1.6925 (1.7831)  loss_ce_unscaled: 0.8608 (0.9116)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.6096 (0.6682)  loss_giou_unscaled: 0.8501 (0.8931)  cardinality_error_unscaled: 2.5000 (10.9091)  loss_ce_0_unscaled: 0.8398 (0.8757)  loss_bbox_0_unscaled: 0.6049 (0.6707)  loss_giou_0_unscaled: 0.8605 (0.8793)  cardinality_error_0_unscaled: 2.5000 (11.0909)  loss_ce_1_unscaled: 0.7657 (0.8497)  loss_bbox_1_unscaled: 0.6226 (0.6653)  loss_giou_1_unscaled: 0.8767 (0.8844)  cardinality_error_1_unscaled: 2.5000 (10.5455)  loss_ce_2_unscaled: 0.7772 (0.8918)  loss_bbox_2_unscaled: 0.6255 (0.6712)  loss_giou_2_unscaled: 0.8756 (0.8898)  cardinality_error_2_unscaled: 2.5000 (11.0909)  loss_ce_3_unscaled: 0.8100 (0.8511)  loss_bbox_3_unscaled: 0.6223 (0.6686)  loss_giou_3_unscaled: 0.8742 (0.8935)  cardinality_error_3_unscaled: 2.5000 (10.2727)  loss_ce_4_unscaled: 0.8502 (0.8933)  loss_bbox_4_unscaled: 0.6081 (0.6676)  loss_giou_4_unscaled: 0.8463 (0.8916)  cardinality_error_4_unscaled: 2.5000 (10.4545)  time: 0.8521  data: 0.2496  max mem: 2287
Epoch: [0]  [ 20/134]  eta: 0:01:11  lr: 0.000100  class_error: 100.00  loss: 37.8800 (36.7638)  loss_ce: 0.7834 (0.8468)  loss_bbox: 3.6168 (3.4776)  loss_giou: 1.7800 (1.8288)  loss_ce_0: 0.7551 (0.8301)  loss_bbox_0: 3.6403 (3.4663)  loss_giou_0: 1.7855 (1.8004)  loss_ce_1: 0.7476 (0.8172)  loss_bbox_1: 3.5318 (3.4677)  loss_giou_1: 1.7643 (1.8117)  loss_ce_2: 0.7360 (0.8327)  loss_bbox_2: 3.5410 (3.4914)  loss_giou_2: 1.7798 (1.8256)  loss_ce_3: 0.7856 (0.8181)  loss_bbox_3: 3.5365 (3.4880)  loss_giou_3: 1.7634 (1.8285)  loss_ce_4: 0.7955 (0.8280)  loss_bbox_4: 3.5851 (3.4761)  loss_giou_4: 1.7832 (1.8288)  loss_ce_unscaled: 0.7834 (0.8468)  class_error_unscaled: 100.0000 (100.0000)  loss_bbox_unscaled: 0.7234 (0.6955)  loss_giou_unscaled: 0.8900 (0.9144)  cardinality_error_unscaled: 2.0000 (6.9048)  loss_ce_0_unscaled: 0.7551 (0.8301)  loss_bbox_0_unscaled: 0.7281 (0.6933)  loss_giou_0_unscaled: 0.8927 (0.9002)  cardinality_error_0_unscaled: 2.0000 (7.0000)  loss_ce_1_unscaled: 0.7476 (0.8172)  loss_bbox_1_unscaled: 0.7064 (0.6935)  loss_giou_1_unscaled: 0.8822 (0.9059)  cardinality_error_1_unscaled: 2.0000 (6.7143)  loss_ce_2_unscaled: 0.7360 (0.8327)  loss_bbox_2_unscaled: 0.7082 (0.6983)  loss_giou_2_unscaled: 0.8899 (0.9128)  cardinality_error_2_unscaled: 2.0000 (7.0000)  loss_ce_3_unscaled: 0.7856 (0.8181)  loss_bbox_3_unscaled: 0.7073 (0.6976)  loss_giou_3_unscaled: 0.8817 (0.9143)  cardinality_error_3_unscaled: 2.0000 (6.5714)  loss_ce_4_unscaled: 0.7955 (0.8280)  loss_bbox_4_unscaled: 0.7170 (0.6952)  loss_giou_4_unscaled: 0.8916 (0.9144)  cardinality_error_4_unscaled: 2.0000 (6.6667)  time: 0.3813  data: 0.0022  max mem: 2353
Traceback (most recent call last):
  File "C:\Users\chris\Desktop\SUS\detr\main.py", line 266, in <module>
    main(args)
  File "C:\Users\chris\Desktop\SUS\detr\main.py", line 199, in main
    train_stats = train_one_epoch(
  File "C:\Users\chris\Desktop\SUS\detr\engine.py", line 56, in train_one_epoch
    optimizer.step()
  File "C:\Users\chris\venv\lib\site-packages\torch\optim\lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "C:\Users\chris\venv\lib\site-packages\torch\optim\optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\chris\venv\lib\site-packages\torch\autograd\grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "C:\Users\chris\venv\lib\site-packages\torch\optim\adamw.py", line 145, in step
    F.adamw(params_with_grad,
  File "C:\Users\chris\venv\lib\site-packages\torch\optim\_functional.py", line 151, in adamw
    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
KeyboardInterrupt